{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aHTrpANhSoC"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAnqNAzfcVu0"
   },
   "source": [
    "# Emotion prediction with GoEmotions and PRADO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcNoWgG7hvIs"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/models/blob/master/research/seq_flow_lite/demo/colab/emotion_colab.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/models/blob/master/research/seq_flow_lite/demo/colab/emotion_colab.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhekoIrWiSsv"
   },
   "source": [
    "In this tutorial, we will work through training a neural emotion prediction model, using the tensorflow-models PIP package, and Bazel.\n",
    "\n",
    "This tutorial is using GoEmotions, an emotion prediction dataset, available on [TensorFlow TFDS](https://www.tensorflow.org/datasets/catalog/goemotions). We will be training a sequence projection model architecture named PRADO, available on [TensorFlow Model Garden](https://github.com/tensorflow/models/blob/master/research/seq_flow_lite/models/prado.py). Finally, we will examine an application of emotion prediction to emoji suggestions from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grmac7ZYj02a"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aGnloeD1Mfo"
   },
   "source": [
    "### Install Tensorflow 2.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqP5qpAR1W4f"
   },
   "source": [
    "The seq_flow_lite library has been written with the assumption that tensorflow 2.10.0 will be used.  It may be necessary to restart the runtime after installing the correct version of Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hzuq_GVn1nXO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow==2.10.0\n",
      "  Downloading tensorflow-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 578.1 MB 9.2 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from tensorflow==2.10.0) (50.3.1.post20201107)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 125.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.1 MB 125.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 2.0 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.20\n",
      "  Downloading numpy-1.23.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.1 MB 39.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from tensorflow==2.10.0) (20.4)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from tensorflow==2.10.0) (2.10.0)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 457 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from tensorflow==2.10.0) (1.11.2)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.50.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 108.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from tensorflow==2.10.0) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from tensorflow==2.10.0) (1.15.0)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.10.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.27.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 128.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "\u001b[K     |████████████████████████████████| 124 kB 133.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.11,>=2.10\n",
      "  Using cached tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.1.0-py3-none-any.whl (5.8 kB)\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Using cached keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Using cached tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 2.3 MB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from packaging->tensorflow==2.10.0) (2.4.7)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.35.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 128.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.0.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 900 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.14.0-py2.py3-none-any.whl (175 kB)\n",
      "\u001b[K     |████████████████████████████████| 175 kB 129.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 133.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.24.0)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Downloading importlib_metadata-5.0.0-py3-none-any.whl (21 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 135.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4.0)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 2.5 MB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.1.1)\n",
      "Installing collected packages: protobuf, libclang, numpy, opt-einsum, keras-preprocessing, gast, astunparse, grpcio, flatbuffers, tensorflow-io-gcs-filesystem, absl-py, tensorboard-data-server, importlib-metadata, markdown, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, tensorboard, termcolor, keras, tensorflow-estimator, google-pasta, tensorflow\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.8 are installed in '/home/ucn3qn/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script markdown_py is installed in '/home/ucn3qn/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/home/ucn3qn/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script google-oauthlib-tool is installed in '/home/ucn3qn/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tensorboard is installed in '/home/ucn3qn/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/ucn3qn/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed absl-py-1.3.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-22.10.26 gast-0.4.0 google-auth-2.14.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.50.0 importlib-metadata-5.0.0 keras-2.10.0 keras-preprocessing-1.1.2 libclang-14.0.6 markdown-3.4.1 numpy-1.23.4 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.27.0 termcolor-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXaKS_JBbyX1"
   },
   "source": [
    "Update CuDNN.  The version installed on the Colab machines does not play well with Tensorflow 2.10.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ErZoDw_9bwSG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: apt: command not found\n"
     ]
    }
   ],
   "source": [
    "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_mi4NZeeB1l"
   },
   "source": [
    "### Install the TensorFlow Datasets pip package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCk46-HdmIyD"
   },
   "source": [
    "`tensorflow_datasets` is a set of collection of datasets that includes the GoEmotions dataset. We install it with pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mvO0_HcKx0_V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow_datasets\n",
      "  Using cached tensorflow_datasets-4.7.0-py3-none-any.whl (4.7 MB)\n",
      "Collecting tensorflow-metadata\n",
      "  Using cached tensorflow_metadata-1.10.0-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: toml in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from tensorflow_datasets) (0.10.1)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from tensorflow_datasets) (5.4.0)\n",
      "Collecting promise\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: numpy in /sfs/qumulo/qhome/ucn3qn/.local/lib/python3.8/site-packages (from tensorflow_datasets) (1.23.4)\n",
      "Requirement already satisfied: six in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Requirement already satisfied: absl-py in /sfs/qumulo/qhome/ucn3qn/.local/lib/python3.8/site-packages (from tensorflow_datasets) (1.3.0)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /sfs/qumulo/qhome/ucn3qn/.local/lib/python3.8/site-packages (from tensorflow_datasets) (3.19.6)\n",
      "Requirement already satisfied: tqdm in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from tensorflow_datasets) (4.50.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from tensorflow_datasets) (2.24.0)\n",
      "Requirement already satisfied: termcolor in /sfs/qumulo/qhome/ucn3qn/.local/lib/python3.8/site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Collecting etils[epath]\n",
      "  Using cached etils-0.9.0-py3-none-any.whl (140 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Using cached googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.25.11)\n",
      "Requirement already satisfied: typing_extensions; extra == \"epath\" in /sfs/applications/202206/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages (from etils[epath]->tensorflow_datasets) (3.7.4.3)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=6ac9ebb8710e3e17ed2dc265022c4212ed737ef650a7c5f40a80ea94d5f534d5\n",
      "  Stored in directory: /sfs/qumulo/qhome/ucn3qn/.cache/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built promise\n",
      "Installing collected packages: googleapis-common-protos, tensorflow-metadata, promise, dill, etils, tensorflow-datasets\n",
      "\u001b[33m  WARNING: The script tfds is installed in '/home/ucn3qn/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed dill-0.3.6 etils-0.9.0 googleapis-common-protos-1.56.4 promise-2.3 tensorflow-datasets-4.7.0 tensorflow-metadata-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2wqyg-7mbfV"
   },
   "source": [
    "### Install the Sequence Projection Models package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JRZS_aSeINK"
   },
   "source": [
    "Install Bazel: This will allow us to build custom TensorFlow ops used by the PRADO architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "N00X4P229Ppm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We trust you have received the usual lecture from the local System\n",
      "Administrator. It usually boils down to these three things:\n",
      "\n",
      "    #1) Respect the privacy of others.\n",
      "    #2) Think before you type.\n",
      "    #3) With great power comes great responsibility.\n",
      "\n",
      "[sudo] password for ucn3qn: \n",
      "sudo: timed out reading password\n",
      "\n",
      "We trust you have received the usual lecture from the local System\n",
      "Administrator. It usually boils down to these three things:\n",
      "\n",
      "    #1) Respect the privacy of others.\n",
      "    #2) Think before you type.\n",
      "    #3) With great power comes great responsibility.\n",
      "\n",
      "[sudo] password for ucn3qn:   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  4714  100  4714    0     0  10092      0 --:--:-- --:--:-- --:--:-- 10115\n",
      "\n",
      "\n",
      "We trust you have received the usual lecture from the local System\n",
      "Administrator. It usually boils down to these three things:\n",
      "\n",
      "    #1) Respect the privacy of others.\n",
      "    #2) Think before you type.\n",
      "    #3) With great power comes great responsibility.\n",
      "\n",
      "[sudo] password for ucn3qn: \n",
      "\n",
      "We trust you have received the usual lecture from the local System\n",
      "Administrator. It usually boils down to these three things:\n",
      "\n",
      "    #1) Respect the privacy of others.\n",
      "    #2) Think before you type.\n",
      "    #3) With great power comes great responsibility.\n",
      "\n",
      "[sudo] password for ucn3qn: \n",
      "sudo: timed out reading password\n",
      "\n",
      "We trust you have received the usual lecture from the local System\n",
      "Administrator. It usually boils down to these three things:\n",
      "\n",
      "    #1) Respect the privacy of others.\n",
      "    #2) Think before you type.\n",
      "    #3) With great power comes great responsibility.\n",
      "\n",
      "[sudo] password for ucn3qn: \n",
      "sudo: timed out reading password\n"
     ]
    }
   ],
   "source": [
    "!sudo apt install curl gnupg\n",
    "!curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add -\n",
    "!echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list\n",
    "!sudo apt update\n",
    "!sudo apt install bazel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JeSDpZFelL5"
   },
   "source": [
    "Install the library:\n",
    "* `seq_flow_lite` includes the PRADO architecture and custom ops.\n",
    "* We download the code from GitHub, and then build and install the TF and TFLite ops used by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mktlCYcd9iLG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'models'...\n",
      "remote: Enumerating objects: 78535, done.\u001b[K\n",
      "remote: Counting objects: 100% (338/338), done.\u001b[K\n",
      "remote: Compressing objects: 100% (194/194), done.\u001b[K\n",
      "remote: Total 78535 (delta 164), reused 288 (delta 143), pack-reused 78197\u001b[K\n",
      "Receiving objects: 100% (78535/78535), 593.72 MiB | 65.13 MiB/s, done.\n",
      "Resolving deltas: 100% (55787/55787), done.\n",
      "Checking out files: 100% (3203/3203), done.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Processing ./models/research/seq_flow_lite\n",
      "Building wheels for collected packages: seq-flow-lite\n",
      "  Building wheel for seq-flow-lite (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-1ikl8boq/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-1ikl8boq/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-a83g7m_s\n",
      "       cwd: /tmp/pip-req-build-1ikl8boq/\n",
      "  Complete output (40 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running bazel_build\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/tmp/pip-req-build-1ikl8boq/setup.py\", line 44, in <module>\n",
      "      setuptools.setup(\n",
      "    File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages/setuptools/__init__.py\", line 153, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/core.py\", line 148, in setup\n",
      "      dist.run_commands()\n",
      "    File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/dist.py\", line 966, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages/wheel/bdist_wheel.py\", line 290, in run\n",
      "      self.run_command('build')\n",
      "    File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/command/build.py\", line 135, in run\n",
      "      self.run_command(cmd_name)\n",
      "    File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/tmp/pip-req-build-1ikl8boq/setup.py\", line 39, in run\n",
      "      subprocess.check_call(\n",
      "    File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/subprocess.py\", line 359, in check_call\n",
      "      retcode = call(*popenargs, **kwargs)\n",
      "    File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/subprocess.py\", line 340, in call\n",
      "      with Popen(*popenargs, **kwargs) as p:\n",
      "    File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/subprocess.py\", line 858, in __init__\n",
      "      self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/subprocess.py\", line 1631, in _execute_child\n",
      "      executable = os.fsencode(executable)\n",
      "    File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/os.py\", line 806, in fsencode\n",
      "      filename = fspath(filename)  # Does type-checking of `filename`.\n",
      "  TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for seq-flow-lite\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for seq-flow-lite\n",
      "Failed to build seq-flow-lite\n",
      "Installing collected packages: seq-flow-lite\n",
      "    Running setup.py install for seq-flow-lite ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-1ikl8boq/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-1ikl8boq/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-64vyzcnq/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/ucn3qn/.local/include/python3.8/seq-flow-lite\n",
      "         cwd: /tmp/pip-req-build-1ikl8boq/\n",
      "    Complete output (42 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running bazel_build\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-req-build-1ikl8boq/setup.py\", line 44, in <module>\n",
      "        setuptools.setup(\n",
      "      File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages/setuptools/__init__.py\", line 153, in setup\n",
      "        return distutils.core.setup(**attrs)\n",
      "      File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/core.py\", line 148, in setup\n",
      "        dist.run_commands()\n",
      "      File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/dist.py\", line 966, in run_commands\n",
      "        self.run_command(cmd)\n",
      "      File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/site-packages/setuptools/command/install.py\", line 61, in run\n",
      "        return orig.install.run(self)\n",
      "      File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/command/install.py\", line 545, in run\n",
      "        self.run_command('build')\n",
      "      File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n",
      "        self.distribution.run_command(command)\n",
      "      File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/command/build.py\", line 135, in run\n",
      "        self.run_command(cmd_name)\n",
      "      File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n",
      "        self.distribution.run_command(command)\n",
      "      File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"/tmp/pip-req-build-1ikl8boq/setup.py\", line 39, in run\n",
      "        subprocess.check_call(\n",
      "      File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/subprocess.py\", line 359, in check_call\n",
      "        retcode = call(*popenargs, **kwargs)\n",
      "      File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/subprocess.py\", line 340, in call\n",
      "        with Popen(*popenargs, **kwargs) as p:\n",
      "      File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/subprocess.py\", line 858, in __init__\n",
      "        self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "      File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/subprocess.py\", line 1631, in _execute_child\n",
      "        executable = os.fsencode(executable)\n",
      "      File \"/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/lib/python3.8/os.py\", line 806, in fsencode\n",
      "        filename = fspath(filename)  # Does type-checking of `filename`.\n",
      "    TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2020.11-py3.8/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-1ikl8boq/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-1ikl8boq/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-64vyzcnq/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/ucn3qn/.local/include/python3.8/seq-flow-lite Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!git clone https://www.github.com/tensorflow/models\n",
    "!models/research/seq_flow_lite/demo/colab/setup_workspace.sh\n",
    "!pip install models/research/seq_flow_lite\n",
    "!rm -rf models/research/seq_flow_lite/tf_ops\n",
    "!rm -rf models/research/seq_flow_lite/tflite_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rP8iKa4Il4mL"
   },
   "source": [
    "## Training an Emotion Prediction Model\n",
    "\n",
    "* First, we load the GoEmotions data from TFDS.\n",
    "* Next, we prepare the PRADO model for training. We set up the model configuration, including hyperparameters and labels. We also prepare the dataset, which involves projecting the inputs from the dataset, and passing the projections to the model.  This is needed because a model training on TPU can not handle string inputs.\n",
    "* Finally, we train and evaluate the model and produce model-level and per-label metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtvR40K8K0Bn"
   },
   "source": [
    "***Start here on Runtime reset***, once the packages above are properly installed:\n",
    "* Go to the `seq_flow_lite` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ImEejssVKvxR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/gpfs0/project/ds6050_FA22/group5/GoEmotions/models/research/seq_flow_lite\n"
     ]
    }
   ],
   "source": [
    "%cd models/research/seq_flow_lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwSPqHXAeQ6H"
   },
   "source": [
    "* Import the Tensorflow and Tensorflow Dataset libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kc4y4n80eL_b"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5ef98d34c8c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-CtG3cagPgl"
   },
   "source": [
    "### The data: GoEmotions\n",
    "In this tutorial, we use the [GoEmotions dataset from TFDS](https://www.tensorflow.org/datasets/catalog/goemotions).\n",
    "\n",
    "GoEmotions is a corpus of comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.\n",
    "\n",
    "*   Number of labels: 27.\n",
    "*   Size of training dataset: 43,410.\n",
    "*   Size of evaluation dataset: 5,427.\n",
    "*   Maximum sequence length in training and evaluation datasets: 30.\n",
    "\n",
    "The emotion categories are admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bvsn_s3S0SAt"
   },
   "source": [
    "Load the data from TFDS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtTLwtEqwcR2"
   },
   "outputs": [],
   "source": [
    "ds = tfds.load('goemotions', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJuu4jKet9zq"
   },
   "source": [
    "Print 5 sample data elements from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0O18rSLuDx5"
   },
   "outputs": [],
   "source": [
    "for element in ds.take(5):\n",
    "  print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAz-tdQfuVBn"
   },
   "source": [
    "### The model: PRADO\n",
    "\n",
    "We train an Emotion Prediction model, based on the [PRADO architecture](https://github.com/tensorflow/models/blob/master/research/seq_flow_lite/models/prado.py) from the [Sequence Projection Models package](https://github.com/tensorflow/models/tree/master/research/seq_flow_lite).\n",
    "\n",
    "PRADO projects input sequences to fixed sized features. The idea behind this approach is to build embedding-free models that minimize the model size. Instead of using an embedding table to lookup embeddings, sequence projection models compute them on the fly, resulting in space-efficient models.\n",
    "\n",
    "In this section, we prepare the PRADO model for training.\n",
    "\n",
    "This GoEmotions dataset is not set up so that it can be directly fed into the PRADO model, so below, we also handle the necessary preprocessing by providing a dataset builder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9uPSZYpgBqP"
   },
   "source": [
    "Prepare the model configuration:\n",
    "* Enumerate the labels expected to be found in the GoEmotions dataset.\n",
    "* Prepare the `MODEL_CONFIG` dictionary which includes training parameters for the model. See sample configs for the PRADO model [here](https://github.com/tensorflow/models/tree/master/research/seq_flow_lite/configs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkQMnTcLyFeR"
   },
   "outputs": [],
   "source": [
    "LABELS = [\n",
    "    'admiration',\n",
    "    'amusement',\n",
    "    'anger',\n",
    "    'annoyance',\n",
    "    'approval',\n",
    "    'caring',\n",
    "    'confusion',\n",
    "    'curiosity',\n",
    "    'desire',\n",
    "    'disappointment',\n",
    "    'disapproval',\n",
    "    'disgust',\n",
    "    'embarrassment',\n",
    "    'excitement',\n",
    "    'fear',\n",
    "    'gratitude',\n",
    "    'grief',\n",
    "    'joy',\n",
    "    'love',\n",
    "    'nervousness',\n",
    "    'optimism',\n",
    "    'pride',\n",
    "    'realization',\n",
    "    'relief',\n",
    "    'remorse',\n",
    "    'sadness',\n",
    "    'surprise',\n",
    "    'neutral',\n",
    "]\n",
    "\n",
    "# Model training parameters.\n",
    "CONFIG = {\n",
    "    'name': 'models.prado',\n",
    "    'batch_size': 1024,\n",
    "    'train_steps': 10000,\n",
    "    'learning_rate': 0.0006,\n",
    "    'learning_rate_decay_steps': 340,\n",
    "    'learning_rate_decay_rate': 0.7,\n",
    "}\n",
    "\n",
    "# Limits the amount of logging output produced by the training run, in order to\n",
    "# avoid browser slowdowns.\n",
    "CONFIG['save_checkpoints_steps'] = int(CONFIG['train_steps'] / 10)\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    'labels': LABELS,\n",
    "    'multilabel': True,\n",
    "    'quantize': False,\n",
    "    'max_seq_len': 128,\n",
    "    'max_seq_len_inference': 128,\n",
    "    'exclude_nonalphaspace_unicodes': False,\n",
    "    'split_on_space': True,\n",
    "    'embedding_regularizer_scale': 0.035,\n",
    "    'embedding_size': 64,\n",
    "    'bigram_channels': 64,\n",
    "    'trigram_channels': 64,\n",
    "    'feature_size': 512,\n",
    "    'network_regularizer_scale': 0.0001,\n",
    "    'keep_prob': 0.5,\n",
    "    'word_novelty_bits': 0,\n",
    "    'doc_size_levels': 0,\n",
    "    'add_bos_tag': False,\n",
    "    'add_eos_tag': False,\n",
    "    'pre_logits_fc_layers': [],\n",
    "    'text_distortion_probability': 0.0,\n",
    "}\n",
    "\n",
    "CONFIG['model_config'] = MODEL_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-pUW649gfzA"
   },
   "source": [
    "Write a function that builds the datasets for the model.  It will load the data, handle batching, and generate projections for the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unYlUYXq119f"
   },
   "outputs": [],
   "source": [
    "from layers import base_layers\n",
    "from layers import projection_layers\n",
    "\n",
    "def build_dataset(mode, inspect=False):\n",
    "  if mode == base_layers.TRAIN:\n",
    "    split = 'train'\n",
    "    count = None\n",
    "  elif mode == base_layers.EVAL:\n",
    "    split = 'test'\n",
    "    count = 1\n",
    "  else:\n",
    "    raise ValueError('mode={}, must be TRAIN or EVAL'.format(mode))\n",
    "\n",
    "  batch_size = CONFIG['batch_size']\n",
    "  if inspect:\n",
    "    batch_size = 1\n",
    "\n",
    "  # Convert examples from their dataset format into the model format.\n",
    "  def process_input(features):\n",
    "    # Generate the projection for each comment_text input.  The final tensor \n",
    "    # will have the shape [batch_size, number of tokens, feature size].\n",
    "    # Additionally, we generate a tensor containing the number of tokens for\n",
    "    # each comment_text (seq_length).  This is needed because the projection\n",
    "    # tensor is a full tensor, and we are not using EOS tokens.\n",
    "    text = features['comment_text']\n",
    "    text = tf.reshape(text, [batch_size])\n",
    "    projection_layer = projection_layers.ProjectionLayer(MODEL_CONFIG, mode)\n",
    "    projection, seq_length = projection_layer(text)\n",
    "\n",
    "    # Convert the labels into an indicator tensor, using the LABELS indices.\n",
    "    label = tf.stack([features[label] for label in LABELS], axis=-1)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    label = tf.reshape(label, [batch_size, len(LABELS)])\n",
    "\n",
    "    model_features = ({'projection': projection, 'sequence_length': seq_length}, label)\n",
    "\n",
    "    if inspect:\n",
    "      model_features = (model_features[0], model_features[1], features)\n",
    "\n",
    "    return model_features\n",
    "\n",
    "  ds = tfds.load('goemotions', split=split)\n",
    "  ds = ds.repeat(count=count)\n",
    "  ds = ds.shuffle(buffer_size=batch_size * 2)\n",
    "  ds = ds.batch(batch_size, drop_remainder=True)\n",
    "  ds = ds.map(process_input,\n",
    "              num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "              deterministic=False)\n",
    "  ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "  return ds\n",
    "\n",
    "train_dataset = build_dataset(base_layers.TRAIN)\n",
    "test_dataset = build_dataset(base_layers.EVAL)\n",
    "inspect_dataset = build_dataset(base_layers.TRAIN, inspect=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQmYWg6ivCHS"
   },
   "source": [
    "Print a batch of examples in model format.  This will consist of:\n",
    "* the projection tensors (projection and seq_length)\n",
    "* the label tensor (second tuple value)\n",
    "\n",
    "The projection tensor is a **[batch size, max_seq_length, feature_size]** floating point tensor.  The **[b, i]** vector is a feature vector of the **i**th token of the **b**th comment_text.  The rest of the tensor is zero-padded, and the\n",
    "seq_length tensor indicates the number of features vectors for each comment_text.\n",
    "\n",
    "The label tensor is an indicator tensor of the set of true labels for the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OyK7rjTvBjF"
   },
   "outputs": [],
   "source": [
    "example = next(iter(train_dataset))\n",
    "print(\"inputs = {}\".format(example[0]))\n",
    "print(\"labels = {}\".format(example[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytMQHT5Kd7A_"
   },
   "source": [
    "In this version of the dataset, the original example has been added as the third element of the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29EzRoCfI91r"
   },
   "outputs": [],
   "source": [
    "example = next(iter(inspect_dataset))\n",
    "print(\"inputs = {}\".format(example[0]))\n",
    "print(\"labels = {}\".format(example[1]))\n",
    "print(\"original example = {}\".format(example[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLDbHTIvvX11"
   },
   "source": [
    "### Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqUTa7wXsHoO"
   },
   "source": [
    "First we define a function to build the model.  We vary the model inputs depending on task.  For training and evaluation, we'll take the projection and sequence length as inputs.  Otherwise, we'll take strings as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erEiNX3ToLZ1"
   },
   "outputs": [],
   "source": [
    "from models import prado\n",
    "\n",
    "def build_model(mode):\n",
    "  # First we define our inputs.\n",
    "  inputs = []\n",
    "  if mode == base_layers.TRAIN or mode == base_layers.EVAL:\n",
    "    # For TRAIN and EVAL, we'll be getting dataset examples,\n",
    "    # so we'll get projections and sequence_lengths.\n",
    "    projection = tf.keras.Input(\n",
    "        shape=(MODEL_CONFIG['max_seq_len'], MODEL_CONFIG['feature_size']),\n",
    "        name='projection',\n",
    "        dtype='float32')\n",
    "\n",
    "    sequence_length = tf.keras.Input(\n",
    "        shape=(), name='sequence_length', dtype='float32')\n",
    "    inputs = [projection, sequence_length]\n",
    "  else:\n",
    "    # Otherwise, we get string inputs which we need to project.\n",
    "    input = tf.keras.Input(shape=(), name='input', dtype='string')\n",
    "    projection_layer = projection_layers.ProjectionLayer(MODEL_CONFIG, mode)\n",
    "    projection, sequence_length = projection_layer(input)\n",
    "    inputs = [input]\n",
    "\n",
    "  # Next we add the model layer.\n",
    "  model_layer = prado.Encoder(MODEL_CONFIG, mode)\n",
    "  logits = model_layer(projection, sequence_length)\n",
    "\n",
    "  # Finally we add an activation layer.\n",
    "  if MODEL_CONFIG['multilabel']:\n",
    "    activation = tf.keras.layers.Activation('sigmoid', name='predictions')\n",
    "  else:\n",
    "    activation = tf.keras.layers.Activation('softmax', name='predictions')\n",
    "  predictions = activation(logits)\n",
    "\n",
    "  model = tf.keras.Model(\n",
    "      inputs=inputs,\n",
    "      outputs=[predictions])\n",
    "  \n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caHpK9Htv40g"
   },
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xM-2R38kogo"
   },
   "outputs": [],
   "source": [
    "# Remove any previous training data.\n",
    "!rm -rf model\n",
    "\n",
    "model = build_model(base_layers.TRAIN)\n",
    "\n",
    "# Create the optimizer.\n",
    "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=CONFIG['learning_rate'],\n",
    "    decay_rate=CONFIG['learning_rate_decay_rate'],\n",
    "    decay_steps=CONFIG['learning_rate_decay_steps'],\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Define the loss function.\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "epochs = int(CONFIG['train_steps'] / CONFIG['save_checkpoints_steps'])\n",
    "model.fit(\n",
    "    x=train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_dataset,\n",
    "    steps_per_epoch=CONFIG['save_checkpoints_steps'])\n",
    "\n",
    "model.save_weights('model/model_checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hdbXBs0g3oX"
   },
   "source": [
    "Load a training checkpoint and evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1qc9GNtF3s5"
   },
   "outputs": [],
   "source": [
    "model = build_model(base_layers.EVAL)\n",
    "\n",
    "# Define metrics over each category.\n",
    "metrics = []\n",
    "for i, label in enumerate(LABELS):\n",
    "  metric = tf.keras.metrics.Precision(\n",
    "      thresholds=[0.5],\n",
    "      class_id=i,\n",
    "      name='precision@0.5/{}'.format(label))\n",
    "  metrics.append(metric)\n",
    "  metric = tf.keras.metrics.Recall(\n",
    "      thresholds=[0.5],\n",
    "      class_id=i,\n",
    "      name='recall@0.5/{}'.format(label))\n",
    "  metrics.append(metric)\n",
    "\n",
    "# Define metrics over the entire task.\n",
    "metric = tf.keras.metrics.Precision(thresholds=[0.5], name='precision@0.5/all')\n",
    "metrics.append(metric)\n",
    "metric = tf.keras.metrics.Recall(thresholds=[0.5], name='recall@0.5/all')\n",
    "metrics.append(metric)\n",
    "\n",
    "model.compile(metrics=metrics)\n",
    "model.load_weights('model/model_checkpoint')\n",
    "result = model.evaluate(x=test_dataset, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Namwa3enwQBc"
   },
   "source": [
    "Print evaluation metrics for the model, as well as per emotion label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l420PosisfXN"
   },
   "outputs": [],
   "source": [
    "for label in LABELS:\n",
    "  precision_key = 'precision@0.5/{}'.format(label)\n",
    "  recall_key = 'recall@0.5/{}'.format(label)\n",
    "  if precision_key in result and recall_key in result:\n",
    "    print('{}: (precision@0.5: {}, recall@0.5: {})'.format(\n",
    "        label, result[precision_key], result[recall_key]))\n",
    "    \n",
    "precision_key = 'precision@0.5/all'\n",
    "recall_key = 'recall@0.5/all'\n",
    "if precision_key in result and recall_key in result:\n",
    "  print('all: (precision@0.5: {}, recall@0.5: {})'.format(\n",
    "      result[precision_key], result[recall_key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZSWnwTMqZ5f"
   },
   "source": [
    "## Suggest Emojis using an Emotion Prediction model\n",
    "\n",
    "In this section, we apply the Emotion Prediction model trained above to suggest emojis relevant to input text.\n",
    "\n",
    "Refer to our [GoEmotions Model Card](https://github.com/google-research/google-research/blob/master/goemotions/goemotions_model_card.pdf) for additional uses of the model and considerations and limitations for using the GoEmotions data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aybpGQV1qr8I"
   },
   "source": [
    "Map each emotion label to a relevant emoji:\n",
    "* Emotions are subtle and multi-faceted. In many cases, no one emoji can truely capture the full complexity of the human experience behind each emotion. \n",
    "* For the purpose of this exercise, we will select an emoji that captures at least one facet that is conveyed by an emotion label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgs12b90qmSQ"
   },
   "outputs": [],
   "source": [
    "EMOJI_MAP = {\n",
    "    'admiration': '👏',\n",
    "    'amusement': '😂',\n",
    "    'anger': '😡',\n",
    "    'annoyance': '😒',\n",
    "    'approval': '👍',\n",
    "    'caring': '🤗',\n",
    "    'confusion': '😕',\n",
    "    'curiosity': '🤔',\n",
    "    'desire': '😍',\n",
    "    'disappointment': '😞',\n",
    "    'disapproval': '👎',\n",
    "    'disgust': '🤮',\n",
    "    'embarrassment': '😳',\n",
    "    'excitement': '🤩',\n",
    "    'fear': '😨',\n",
    "    'gratitude': '🙏',\n",
    "    'grief': '😢',\n",
    "    'joy': '😃',\n",
    "    'love': '❤️',\n",
    "    'nervousness': '😬',\n",
    "    'optimism': '🤞',\n",
    "    'pride': '😌',\n",
    "    'realization': '💡',\n",
    "    'relief': '😅',\n",
    "    'remorse': '',\n",
    "    'sadness': '😞',\n",
    "    'surprise': '😲',\n",
    "    'neutral': '',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rh_3y7OL7JG_"
   },
   "source": [
    "Select sample inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdD6xPpn7Mjm"
   },
   "outputs": [],
   "source": [
    "PREDICT_TEXT = [\n",
    "  b'Good for you!',\n",
    "  b'Happy birthday!',\n",
    "  b'I love you.',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vavivya6hGw0"
   },
   "source": [
    "Run inference for the selected examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJ6iyLlLo5-3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model = build_model(base_layers.PREDICT)\n",
    "model.load_weights('model/model_checkpoint')\n",
    "\n",
    "for text in PREDICT_TEXT:\n",
    "  results = model.predict(x=[text])\n",
    "  print('')\n",
    "  print('{}:'.format(text))\n",
    "  labels = np.flip(np.argsort(results[0]))\n",
    "  for x in range(3):\n",
    "    label = LABELS[labels[x]]\n",
    "    label = EMOJI_MAP[label] if EMOJI_MAP[label] else label\n",
    "    print('{}: {}'.format(label, results[0][labels[x]]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Emotion prediction with GoEmotions and PRADO",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
